---
prev: 9
next: 11
---

# Customization

One great thing about the Linux command line is how easy it is to customize.  For example, submitting an array of R jobs like the one on the previous page is something I do all the time, so I simply added the following commands to `~/bin/`:

{% include pre-file.html file="rbatch" %}
#!/bin/bash
R CMD BATCH --no-save --no-restore "--args $SGE_TASK_ID $SGE_TASK_LAST" $1 .$1.Rout
{% include post-file.html %}

{% include pre-file.html file="qbatch" %}
#!/bin/bash
qsub -cwd -V -e ~/err -o ~/out -q BIOSTAT -t 1-$2 ~/bin/rbatch $1
{% include post-file.html %}

Now, we can submit arrays of R jobs from the command line without writing any
extra scripts like `sim` and `batch-sim` (which we will do shortly).

Furthermore, it's extremely useful to have modular, versatile code.  In
particular, I don't like writing R code, then rewriting R code to run on the
cluster, then re-re-writing it if I want to run it again on my machine.  All of
these rewrites are (a) annoying and (b) an opportunity to make a mistake.  For
example, comparing the versions of `sim.R` [here](i.html) and [here](ii.html),
you'll note that one of them works in an interactive session but not a batch
session, while the other works in a batch session, but won't run in an
interactive session.  To avoid switching between the two, I use a simple R
function I called `Bsave` for "batch save" (source code [here](misc/Bsave.R) and
[here](misc/Bgather.R) for its cousin, `Bgather`; add `source("Bsave.R")` to
your `~/.Rprofile` file to make this available in your R sessions).  To see why
this is useful, let's watch it in action.  Let's rewrite `sim.R` one last time:

{% include pre-file.html file="sim.R" %}
N <- 10000
p <- numeric(N)
n <- 10
for (i in 1:N) {
  x <- rnorm(n)
  y <- rnorm(n, sd=3)
  p[i] <- t.test(x, y, var.equal=TRUE)$p.value
}
Bsave(p)
{% include post-file.html %}

Finally, I also have a command, `gather`, that simply opens R, calls `Bgather()`, and then closes R:

{% include pre-file.html file="gather" %}
#!/bin/bash
Rscript -e 'Bgather()'
{% include post-file.html %}

Now let's see what these functions do.  If we run this in an interactive session of R, `p` will be saved in a file named with today's date.  When we run it non-interactively on the cluster with

{% include pre-prompt.html %}
qbatch sim.R 10
{% include post-prompt.html %}

The results are saved in `tmp1.RData`, `tmp2.RData`, and so on.  To combine them, we submit:

{% include pre-prompt.html %}
gather
{% include post-prompt.html %}

Now all the tmp files are gone and we are left with a file `2017-02-21.RData` (or whatever the date is).  If you load it, you'll see that it contains all 100,000 results.  In this particular example, the result was a scalar, but Bsave/Bgather work for an array of any dimensions, provided that the first dimension is the one we're merging on.  This is about as low a barrier as you can hope for, short of running jobs on multiple processors from within R itself: no need to modify any code or to write any scripts, just run `qbatch` and then `Bgather()` when you're done.

This has just been an illustration of some personal things I've done to customize Argon and make transitioning code to and from Argon go smoothly.  You're of course welcome to use these tools yourself, but my main reason for showing this was to demonstrate the kinds of customization possible so that you can think about the kind of helping scripts you might want to write to make your work easier.
