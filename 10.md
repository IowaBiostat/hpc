---
prev: 9
next: 11
---

# Customization

One great thing about the Linux command line is how easy it is to customize.  For example, submitting an array of R jobs like the one on the previous page is something I do all the time, so I simply added the following commands to `~/bin/`:

{% include pre-file.html file="rbatch" %}
#!/bin/bash
R CMD BATCH --no-save --no-restore "--args $SGE_TASK_ID $SGE_TASK_LAST" $1 .$1.Rout
{% include post-file.html %}

{% include pre-file.html file="qbatch" %}
#!/bin/bash
qsub -cwd -V -e ~/err -o ~/out -q BIOSTAT -t 1-$2 ~/bin/rbatch $1
{% include post-file.html %}

Now, we can submit arrays of R jobs from the command line without writing any
extra scripts like `sim` and `batch-sim` (which we will do shortly).

Furthermore, it's extremely useful to have modular, versatile code.  In
particular, I don't like writing R code, then rewriting R code to run on the
cluster, then re-re-writing it if I want to run it again on my machine.  All of
these rewrites are (a) annoying and (b) an opportunity to make a mistake.  For
example, comparing the versions of `sim.R` [here](i.html) and [here](ii.html),
you'll note that one of them works in an interactive session but not a batch
session, while the other works in a batch session, but won't run in an
interactive session.  To avoid switching between the two, I use a pair of simple R functions:
* [`Bsave()`](misc/bsave.r): This is a function called within R to save results with a structured naming convention that is easy to combine later.  To make this available, you can add `source("bsave.r")` to your `~/.Rprofile` file.
* [gather](misc/gather): This is a command-line script that combines multiple files (one from each job) into one file that contains all the results (to do the gathering, it uses the `abind` package, which you will have to install).  To make this available, add it to the directory where you keep your executable files (`~/bin`, if you're following the naming convention used in this tutorial) and make sure it is executable (`chmod u+x`).

To see why these functions are useful, let's watch them in action.  Let's
rewrite `sim.R` one last time:

{% include pre-file.html file="sim.R" %}
N <- 10000
p <- numeric(N)
n <- 10
for (i in 1:N) {
  x <- rnorm(n)
  y <- rnorm(n, sd=3)
  p[i] <- t.test(x, y, var.equal=TRUE)$p.value
}
Bsave(p)
{% include post-file.html %}

If we run this in an interactive session of R, `p` will be saved in a file named with today's date.  When we run it non-interactively on the cluster with

{% include pre-prompt.html %}
qbatch sim.R 10
{% include post-prompt.html %}

The results are saved in `tmp1.rds`, `tmp2.rds`, and so on.  To combine them, submit:

{% include pre-prompt.html %}
gather
{% include post-prompt.html %}

Now all the tmp files are gone and we are left with a file `2017-02-21.rds` (or whatever the date is).  If you load it into R, `p <- readRDS('2017-02-21.rds')`, you'll see that it contains all 100,000 results.  This is about as low a barrier as you can hope for, short of running jobs on multiple processors from within R itself: no need to modify any code or to write any scripts, just run `qbatch` and then `gather` when you're done.

In this particular example, the result was a scalar, but `Bsave()`/`gather` work for an array of any dimensions, provided that the first dimension is the one we're merging on.  You can also add a suffix (e.g., `Bsave(p, 'a')` if, for example, you want to save different results in different files; see the comments in `bsave.r` for documentation and examples.

This has just been an illustration of some personal things I've done to customize Argon and make transitioning code to and from Argon go smoothly.  You're of course welcome to use these tools yourself, but my main reason for showing this was to demonstrate the kinds of customization possible so that you can think about the kind of helping scripts you might want to write to make your work easier.
